{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ce0088f",
   "metadata": {},
   "source": [
    "# GPU timing, batching, and PPL inference (extension of gradient tutorial)\n",
    "\n",
    "This notebook extends the gradient-based transmission-string workflow with:\n",
    "\n",
    "1. Single-run GPU timing and NumPyro/NUTS inference for one light curve.\n",
    "2. Batched GPU timing and NumPyro/NUTS inference for 10 light curves with different `r0` and `rn_frac` values.\n",
    "\n",
    "It follows the same model style used in the gradient-based inference tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a5c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import arviz as az\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "\n",
    "from harmonica import HarmonicaTransit\n",
    "from harmonica.jax import harmonica_transit_quad_ld\n",
    "from harmonica.jax.custom_primitives import harmonica_transit_quad_ld_batch\n",
    "from numpyro.infer import MCMC, NUTS, init_to_median\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3a1b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = [d for d in jax.devices() if d.platform == \"gpu\"]\n",
    "if not gpu_devices:\n",
    "    raise RuntimeError(\n",
    "        \"No GPU detected by JAX. Install a CUDA-enabled jax/jaxlib build and rerun this notebook.\"\n",
    "    )\n",
    "\n",
    "gpu_device = gpu_devices[0]\n",
    "print(\"JAX backend:\", jax.default_backend())\n",
    "print(\"Selected GPU device:\", gpu_device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e300cc",
   "metadata": {},
   "source": [
    "## Shared synthetic setup (same orbit/LD style as gradient tutorial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43f63a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "\n",
    "times = np.linspace(-0.15, 0.15, 500)\n",
    "theta = np.linspace(-np.pi, np.pi, 1000)\n",
    "\n",
    "# Same transmission-string construction pattern as the gradient tutorial.\n",
    "r_mean = np.array([0.15])\n",
    "r_dev = np.random.uniform(-0.1, 0.1, size=6)\n",
    "injected_r = np.concatenate([r_mean, r_dev * r_mean])\n",
    "\n",
    "ht = HarmonicaTransit(times)\n",
    "ht.set_orbit(t0=0.0, period=4.0, a=11.0, inc=87.0 * np.pi / 180.0)\n",
    "ht.set_stellar_limb_darkening(np.array([0.027, 0.246]), limb_dark_law=\"quadratic\")\n",
    "ht.set_planet_transmission_string(injected_r)\n",
    "\n",
    "injected_transmission_string = ht.get_planet_transmission_string(theta)\n",
    "flux_sigma_single = 500.0e-6 * np.ones(times.shape[0])\n",
    "flux_errs_single = np.random.normal(loc=0.0, scale=flux_sigma_single, size=times.shape[0])\n",
    "observed_flux_single = ht.get_transit_light_curve() + flux_errs_single\n",
    "\n",
    "times_jax = jnp.asarray(times)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6972b67",
   "metadata": {},
   "source": [
    "## Single light-curve GPU timing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1197b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_gpu_fn = jax.jit(\n",
    "    lambda r: harmonica_transit_quad_ld(\n",
    "        times_jax,\n",
    "        t0=0.0,\n",
    "        period=4.0,\n",
    "        a=11.0,\n",
    "        inc=87.0 * np.pi / 180.0,\n",
    "        ecc=0.0,\n",
    "        omega=0.0,\n",
    "        u1=0.027,\n",
    "        u2=0.246,\n",
    "        r=r,\n",
    "    )\n",
    ")\n",
    "\n",
    "with jax.default_device(gpu_device):\n",
    "    t0_compile = time.perf_counter()\n",
    "    _ = single_gpu_fn(jnp.asarray(injected_r)).block_until_ready()\n",
    "    compile_plus_first_ms_single = (time.perf_counter() - t0_compile) * 1.0e3\n",
    "\n",
    "with jax.default_device(gpu_device):\n",
    "    t0_run = time.perf_counter()\n",
    "    single_flux_gpu = single_gpu_fn(jnp.asarray(injected_r))\n",
    "    single_flux_gpu.block_until_ready()\n",
    "    run_ms_single = (time.perf_counter() - t0_run) * 1.0e3\n",
    "\n",
    "print(f\"Single compile + first run: {compile_plus_first_ms_single:.3f} ms\")\n",
    "print(f\"Single steady-state GPU run: {run_ms_single:.3f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7b9981",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(times, observed_flux_single, yerr=flux_sigma_single, fmt=\".k\", alpha=0.3, label=\"Noisy observations\")\n",
    "plt.plot(times, np.asarray(single_flux_gpu), color=cm.BuGn(0.8), lw=2.0, label=\"GPU model\")\n",
    "plt.xlabel(\"Time / days\")\n",
    "plt.ylabel(\"Relative flux\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deb5826",
   "metadata": {},
   "source": [
    "## Single light-curve PPL model (NumPyro/NUTS)\n",
    "\n",
    "This mirrors the gradient tutorial model: infer `r0` and `rn_frac`, and build\n",
    "`r = [r0, rn_frac * r0]` before calling Harmonica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpyro_model_single(t, flux_sigma, f_obs=None):\n",
    "    r0 = numpyro.sample(\"r0\", dist.Uniform(0.15 - 0.05, 0.15 + 0.05))\n",
    "    rn_frac = numpyro.sample(\"rn_frac\", dist.Normal(0.0, 0.1), sample_shape=(6,))\n",
    "    r = numpyro.deterministic(\"r\", jnp.concatenate([jnp.array([r0]), rn_frac * r0]))\n",
    "\n",
    "    fs = harmonica_transit_quad_ld(\n",
    "        t,\n",
    "        t0=0.0,\n",
    "        period=4.0,\n",
    "        a=11.0,\n",
    "        inc=87.0 * np.pi / 180.0,\n",
    "        u1=0.027,\n",
    "        u2=0.246,\n",
    "        r=r,\n",
    "    )\n",
    "\n",
    "    numpyro.sample(\"obs\", dist.Normal(fs, flux_sigma), obs=f_obs)\n",
    "\n",
    "nuts_single = NUTS(\n",
    "    numpyro_model_single,\n",
    "    dense_mass=True,\n",
    "    adapt_mass_matrix=True,\n",
    "    max_tree_depth=7,\n",
    "    target_accept_prob=0.75,\n",
    "    init_strategy=init_to_median(),\n",
    ")\n",
    "\n",
    "mcmc_single = MCMC(\n",
    "    nuts_single,\n",
    "    num_warmup=200,\n",
    "    num_samples=500,\n",
    "    num_chains=1,\n",
    "    chain_method=\"sequential\",\n",
    "    progress_bar=True,\n",
    ")\n",
    "\n",
    "with jax.default_device(gpu_device):\n",
    "    t0_ppl = time.perf_counter()\n",
    "    mcmc_single.run(\n",
    "        jax.random.PRNGKey(2),\n",
    "        jnp.asarray(times),\n",
    "        flux_sigma=jnp.asarray(flux_sigma_single),\n",
    "        f_obs=jnp.asarray(observed_flux_single),\n",
    "    )\n",
    "    ppl_single_s = time.perf_counter() - t0_ppl\n",
    "\n",
    "print(f\"Single-curve NumPyro run time: {ppl_single_s:.2f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6fad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_data = az.from_numpyro(mcmc_single)\n",
    "az.summary(single_data, var_names=[\"r0\", \"rn_frac\"], round_to=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4d0a52",
   "metadata": {},
   "source": [
    "## Build 10-curve batch with varied `r0` and `rn_frac`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba68b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "rng = np.random.default_rng(21)\n",
    "\n",
    "# Vary baseline radius across the 10 curves.\n",
    "r0_values_true = np.linspace(0.12, 0.18, batch_size)\n",
    "\n",
    "# Draw six rn/r0 terms for each curve.\n",
    "rn_frac_values_true = rng.normal(0.0, 0.06, size=(batch_size, 6))\n",
    "r_batch_true = np.concatenate(\n",
    "    [r0_values_true[:, None], rn_frac_values_true * r0_values_true[:, None]], axis=1\n",
    ")\n",
    "\n",
    "# Shared orbital/stellar parameters across curves.\n",
    "t0_batch = np.zeros(batch_size)\n",
    "period_batch = np.full(batch_size, 4.0)\n",
    "a_batch = np.full(batch_size, 11.0)\n",
    "inc_batch = np.full(batch_size, 87.0 * np.pi / 180.0)\n",
    "ecc_batch = np.zeros(batch_size)\n",
    "omega_batch = np.zeros(batch_size)\n",
    "u1_batch = np.full(batch_size, 0.027)\n",
    "u2_batch = np.full(batch_size, 0.246)\n",
    "\n",
    "batch_gpu_fn = jax.jit(\n",
    "    lambda r: harmonica_transit_quad_ld_batch(\n",
    "        times_jax,\n",
    "        t0=jnp.asarray(t0_batch),\n",
    "        period=jnp.asarray(period_batch),\n",
    "        a=jnp.asarray(a_batch),\n",
    "        inc=jnp.asarray(inc_batch),\n",
    "        ecc=jnp.asarray(ecc_batch),\n",
    "        omega=jnp.asarray(omega_batch),\n",
    "        u1=jnp.asarray(u1_batch),\n",
    "        u2=jnp.asarray(u2_batch),\n",
    "        r=r,\n",
    "    )\n",
    ")\n",
    "\n",
    "with jax.default_device(gpu_device):\n",
    "    batch_flux_true = batch_gpu_fn(jnp.asarray(r_batch_true))\n",
    "    batch_flux_true.block_until_ready()\n",
    "\n",
    "flux_sigma_batch = 500.0e-6 * np.ones((batch_size, times.shape[0]))\n",
    "flux_errs_batch = rng.normal(loc=0.0, scale=flux_sigma_batch)\n",
    "observed_flux_batch = np.asarray(batch_flux_true) + flux_errs_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc65927b",
   "metadata": {},
   "source": [
    "## Batched GPU timing and visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9fbcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jax.default_device(gpu_device):\n",
    "    t0_compile_batch = time.perf_counter()\n",
    "    _ = batch_gpu_fn(jnp.asarray(r_batch_true)).block_until_ready()\n",
    "    compile_plus_first_ms_batch = (time.perf_counter() - t0_compile_batch) * 1.0e3\n",
    "\n",
    "with jax.default_device(gpu_device):\n",
    "    t0_run_batch = time.perf_counter()\n",
    "    batch_flux_gpu = batch_gpu_fn(jnp.asarray(r_batch_true))\n",
    "    batch_flux_gpu.block_until_ready()\n",
    "    run_ms_batch = (time.perf_counter() - t0_run_batch) * 1.0e3\n",
    "\n",
    "print(f\"Batch compile + first run (B={batch_size}): {compile_plus_first_ms_batch:.3f} ms\")\n",
    "print(f\"Batch steady-state GPU run (B={batch_size}): {run_ms_batch:.3f} ms\")\n",
    "print(\"Batch output shape:\", batch_flux_gpu.shape)\n",
    "\n",
    "batch_flux_np = np.asarray(batch_flux_gpu)\n",
    "rn_frac_rms = np.sqrt(np.mean(rn_frac_values_true**2, axis=1))\n",
    "\n",
    "plt.figure(figsize=(11, 7))\n",
    "for i in range(batch_size):\n",
    "    color = cm.viridis(i / (batch_size - 1))\n",
    "    label = f\"{i + 1}: r0={r0_values_true[i]:.3f}, rn_frac_rms={rn_frac_rms[i]:.3f}\"\n",
    "    plt.plot(times, batch_flux_np[i], color=color, lw=1.7, label=label)\n",
    "\n",
    "plt.xlabel(\"Time / days\")\n",
    "plt.ylabel(\"Relative flux\")\n",
    "plt.title(\"Batched GPU light curves with varying r0 and rn_frac\")\n",
    "plt.legend(loc=\"best\", fontsize=8, ncol=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70992f35",
   "metadata": {},
   "source": [
    "## Batched PPL model (NumPyro/NUTS)\n",
    "\n",
    "This is the batched analog of the single model:\n",
    "\n",
    "- infer `r0` for each of the 10 curves,\n",
    "- infer `rn_frac` for each curve,\n",
    "- build batched `r`,\n",
    "- evaluate `harmonica_transit_quad_ld_batch`,\n",
    "- condition on the 10 observed light curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpyro_model_batch(t, flux_sigma, f_obs=None):\n",
    "    r0 = numpyro.sample(\"r0\", dist.Uniform(0.15 - 0.05, 0.15 + 0.05), sample_shape=(batch_size,))\n",
    "    rn_frac = numpyro.sample(\"rn_frac\", dist.Normal(0.0, 0.1), sample_shape=(batch_size, 6))\n",
    "    r = numpyro.deterministic(\"r\", jnp.concatenate([r0[:, None], rn_frac * r0[:, None]], axis=1))\n",
    "\n",
    "    fs = harmonica_transit_quad_ld_batch(\n",
    "        t,\n",
    "        t0=jnp.asarray(t0_batch),\n",
    "        period=jnp.asarray(period_batch),\n",
    "        a=jnp.asarray(a_batch),\n",
    "        inc=jnp.asarray(inc_batch),\n",
    "        ecc=jnp.asarray(ecc_batch),\n",
    "        omega=jnp.asarray(omega_batch),\n",
    "        u1=jnp.asarray(u1_batch),\n",
    "        u2=jnp.asarray(u2_batch),\n",
    "        r=r,\n",
    "    )\n",
    "\n",
    "    numpyro.sample(\"obs\", dist.Normal(fs, flux_sigma), obs=f_obs)\n",
    "\n",
    "nuts_batch = NUTS(\n",
    "    numpyro_model_batch,\n",
    "    dense_mass=False,\n",
    "    adapt_mass_matrix=True,\n",
    "    max_tree_depth=7,\n",
    "    target_accept_prob=0.75,\n",
    "    init_strategy=init_to_median(),\n",
    ")\n",
    "\n",
    "mcmc_batch = MCMC(\n",
    "    nuts_batch,\n",
    "    num_warmup=150,\n",
    "    num_samples=300,\n",
    "    num_chains=1,\n",
    "    chain_method=\"sequential\",\n",
    "    progress_bar=True,\n",
    ")\n",
    "\n",
    "with jax.default_device(gpu_device):\n",
    "    t0_ppl_batch = time.perf_counter()\n",
    "    mcmc_batch.run(\n",
    "        jax.random.PRNGKey(7),\n",
    "        jnp.asarray(times),\n",
    "        flux_sigma=jnp.asarray(flux_sigma_batch),\n",
    "        f_obs=jnp.asarray(observed_flux_batch),\n",
    "    )\n",
    "    ppl_batch_s = time.perf_counter() - t0_ppl_batch\n",
    "\n",
    "print(f\"Batched NumPyro run time (B={batch_size}): {ppl_batch_s:.2f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0423e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = az.from_numpyro(mcmc_batch)\n",
    "az.summary(batch_data, var_names=[\"r0\"], round_to=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
